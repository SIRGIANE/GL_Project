{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e298509b",
   "metadata": {},
   "source": [
    "# Risk Predictor - Data Exploration and Analysis\n",
    "\n",
    "This notebook provides comprehensive data exploration and analysis for the risk prediction model.\n",
    "\n",
    "## Objectives:\n",
    "- Load and examine the dataset structure\n",
    "- Perform exploratory data analysis (EDA)\n",
    "- Identify patterns and correlations\n",
    "- Visualize key insights\n",
    "- Prepare data quality assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396bb93",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Machine learning preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1423ed6d",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b88096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Update the path according to your data location\n",
    "data_path = '/home/jovyan/data/raw/'  # Docker volume path\n",
    "\n",
    "# Example: Load CSV files (update according to your data format)\n",
    "try:\n",
    "    # Replace with your actual data file\n",
    "    df = pd.read_csv(f'{data_path}risk_data.csv')\n",
    "    print(f'Dataset loaded successfully!')\n",
    "    print(f'Shape: {df.shape}')\n",
    "except FileNotFoundError:\n",
    "    print('Data file not found. Please ensure your data is in the /data/raw/ directory.')\n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    df = pd.DataFrame({\n",
    "        'age': np.random.randint(18, 80, 1000),\n",
    "        'income': np.random.normal(50000, 20000, 1000),\n",
    "        'credit_score': np.random.randint(300, 850, 1000),\n",
    "        'employment_length': np.random.randint(0, 40, 1000),\n",
    "        'loan_amount': np.random.normal(25000, 15000, 1000),\n",
    "        'risk_level': np.random.choice(['Low', 'Medium', 'High'], 1000, p=[0.6, 0.3, 0.1])\n",
    "    })\n",
    "    print('Sample data created for demonstration purposes')\n",
    "\n",
    "# Display basic information\n",
    "print('\\nDataset Info:')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print('First 5 rows:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee94323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print('Statistical Summary:')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421e344",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c080ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('Missing Values:')\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing Count': missing_data.values,\n",
    "    'Missing Percentage': missing_percentage.values\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b49657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f'Number of duplicate rows: {duplicates}')\n",
    "\n",
    "# Data types\n",
    "print('\\nData Types:')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab788f",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8bdb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "if 'risk_level' in df.columns:\n",
    "    fig = px.histogram(df, x='risk_level', title='Distribution of Risk Levels')\n",
    "    fig.show()\n",
    "    \n",
    "    # Count and percentage\n",
    "    risk_counts = df['risk_level'].value_counts()\n",
    "    risk_percentages = df['risk_level'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print('Risk Level Distribution:')\n",
    "    for level in risk_counts.index:\n",
    "        print(f'{level}: {risk_counts[level]} ({risk_percentages[level]:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical variables\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Numerical Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f07bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical variables\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    if i < len(axes):\n",
    "        df[col].hist(bins=30, ax=axes[i], alpha=0.7)\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c41b7",
   "metadata": {},
   "source": [
    "## 5. Risk Analysis by Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numerical variables by risk level\n",
    "if 'risk_level' in df.columns:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        if i < len(axes) and col != 'risk_level':\n",
    "            df.boxplot(column=col, by='risk_level', ax=axes[i])\n",
    "            axes[i].set_title(f'{col} by Risk Level')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeaf497",
   "metadata": {},
   "source": [
    "## 6. Statistical Tests and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for numerical variables vs risk level\n",
    "if 'risk_level' in df.columns:\n",
    "    print('Statistical Tests (ANOVA) for Numerical Variables:')\n",
    "    print('=' * 50)\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        if col != 'risk_level':\n",
    "            groups = [df[df['risk_level'] == level][col].dropna() \n",
    "                     for level in df['risk_level'].unique()]\n",
    "            \n",
    "            f_stat, p_value = stats.f_oneway(*groups)\n",
    "            print(f'{col}:')\n",
    "            print(f'  F-statistic: {f_stat:.4f}')\n",
    "            print(f'  p-value: {p_value:.4f}')\n",
    "            print(f'  Significant: {\"Yes\" if p_value < 0.05 else \"No\"}')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34048c2f",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary insights\n",
    "print('=== DATA EXPLORATION SUMMARY ===')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Missing values: {df.isnull().sum().sum()}')\n",
    "print(f'Duplicate rows: {df.duplicated().sum()}')\n",
    "print(f'Numerical columns: {len(numerical_cols)}')\n",
    "print(f'Categorical columns: {len(df.select_dtypes(include=[\"object\"]).columns)}')\n",
    "\n",
    "if 'risk_level' in df.columns:\n",
    "    print(f'\\nRisk Level Distribution:')\n",
    "    for level, count in df['risk_level'].value_counts().items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f'  {level}: {count} ({pct:.1f}%)')\n",
    "\n",
    "print('\\n=== NEXT STEPS ===')\n",
    "print('1. Data cleaning and preprocessing')\n",
    "print('2. Feature engineering')\n",
    "print('3. Model development and training')\n",
    "print('4. Model evaluation and validation')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
