{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74e49769",
   "metadata": {},
   "source": [
    "# Risk Predictor - Model Development and Training\n",
    "\n",
    "This notebook focuses on building and training machine learning models for risk prediction.\n",
    "\n",
    "## Objectives:\n",
    "- Data preprocessing and feature engineering\n",
    "- Model selection and comparison\n",
    "- Hyperparameter tuning\n",
    "- Model training and validation\n",
    "- Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24583766",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn components\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data (or create sample data)\n",
    "data_path = '/home/jovyan/data/processed/'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(f'{data_path}processed_risk_data.csv')\n",
    "    print(f'Processed data loaded successfully!')\n",
    "except FileNotFoundError:\n",
    "    print('Creating sample dataset for model training...')\n",
    "    # Create more comprehensive sample data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 5000\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'age': np.random.randint(18, 80, n_samples),\n",
    "        'income': np.random.lognormal(10.5, 0.8, n_samples),\n",
    "        'credit_score': np.random.normal(650, 100, n_samples).clip(300, 850),\n",
    "        'employment_length': np.random.exponential(5, n_samples).clip(0, 40),\n",
    "        'loan_amount': np.random.lognormal(10, 0.7, n_samples),\n",
    "        'debt_to_income': np.random.beta(2, 5, n_samples) * 100,\n",
    "        'previous_defaults': np.random.poisson(0.3, n_samples),\n",
    "        'education_level': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "                                          n_samples, p=[0.4, 0.35, 0.2, 0.05])\n",
    "    })\n",
    "    \n",
    "    # Create target variable with realistic relationships\n",
    "    risk_score = (\n",
    "        -0.01 * df['age'] +\n",
    "        -0.00001 * df['income'] +\n",
    "        -0.005 * df['credit_score'] +\n",
    "        -0.02 * df['employment_length'] +\n",
    "        0.00002 * df['loan_amount'] +\n",
    "        0.02 * df['debt_to_income'] +\n",
    "        0.5 * df['previous_defaults'] +\n",
    "        np.random.normal(0, 0.5, n_samples)\n",
    "    )\n",
    "    \n",
    "    df['risk_level'] = pd.cut(risk_score, \n",
    "                             bins=[-np.inf, -0.5, 0.5, np.inf], \n",
    "                             labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb62b0",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0aaa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('risk_level', axis=1)\n",
    "y = df['risk_level']\n",
    "\n",
    "# Identify column types\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f'Numerical features: {numerical_features}')\n",
    "print(f'Categorical features: {categorical_features}')\n",
    "print(f'Target distribution:\\n{y.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17723ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f'Target encoding: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f'Training set size: {X_train.shape}')\n",
    "print(f'Test set size: {X_test.shape}')\n",
    "print(f'Training target distribution: {np.bincount(y_train)}')\n",
    "print(f'Test target distribution: {np.bincount(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f443cd1",
   "metadata": {},
   "source": [
    "## 3. Model Selection and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56792e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Store results\n",
    "model_results = {}\n",
    "\n",
    "print('Comparing models using cross-validation...')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models with cross-validation\n",
    "for name, model in models.items():\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'pipeline': pipeline\n",
    "    }\n",
    "    \n",
    "    print(f'{name}:')\n",
    "    print(f'  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7468f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "model_names = list(model_results.keys())\n",
    "cv_means = [model_results[name]['cv_mean'] for name in model_names]\n",
    "cv_stds = [model_results[name]['cv_std'] for name in model_names]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, cv_means, yerr=cv_stds, capsize=5, alpha=0.7)\n",
    "plt.title('Model Comparison - Cross Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean in zip(bars, cv_means):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "             f'{mean:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['cv_mean'])\n",
    "print(f'Best performing model: {best_model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce3b77",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88241f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the best model\n",
    "best_pipeline = model_results[best_model_name]['pipeline']\n",
    "\n",
    "# Define parameter grid based on best model\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [10, 20, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
    "        'classifier__max_depth': [3, 5, 7]\n",
    "    }\n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    param_grid = {\n",
    "        'classifier__C': [0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear', 'saga']\n",
    "    }\n",
    "else:\n",
    "    param_grid = {}\n",
    "\n",
    "print(f'Tuning hyperparameters for {best_model_name}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bada2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "if param_grid:\n",
    "    grid_search = GridSearchCV(\n",
    "        best_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f'Best parameters: {grid_search.best_params_}')\n",
    "    print(f'Best CV score: {grid_search.best_score_:.4f}')\n",
    "    \n",
    "    # Use the best model\n",
    "    final_model = grid_search.best_estimator_\n",
    "else:\n",
    "    print('Using default parameters for this model')\n",
    "    final_model = best_pipeline\n",
    "    final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89f4ce",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39490aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_proba = final_model.predict_proba(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print('=== MODEL PERFORMANCE ===')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abd419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "target_names = label_encoder.classes_\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53dbfc",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for tree-based models)\n",
    "if hasattr(final_model.named_steps['classifier'], 'feature_importances_'):\n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = (numerical_features + \n",
    "                    list(final_model.named_steps['preprocessor']\n",
    "                         .named_transformers_['cat']\n",
    "                         .get_feature_names_out(categorical_features)))\n",
    "    \n",
    "    importances = final_model.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    feature_imp_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(data=feature_imp_df.head(15), y='feature', x='importance')\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('Top 10 Most Important Features:')\n",
    "    print(feature_imp_df.head(10))\n",
    "else:\n",
    "    print('Feature importance not available for this model type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788371f3",
   "metadata": {},
   "source": [
    "## 7. Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523890a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and preprocessing components\n",
    "model_path = '/home/jovyan/data/models/'\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(final_model, f'{model_path}risk_predictor_model.pkl')\n",
    "joblib.dump(label_encoder, f'{model_path}label_encoder.pkl')\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_type': best_model_name,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'feature_names': list(X.columns),\n",
    "    'target_classes': list(target_names)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{model_path}model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print('Model saved successfully!')\n",
    "print(f'Model file: {model_path}risk_predictor_model.pkl')\n",
    "print(f'Label encoder: {model_path}label_encoder.pkl')\n",
    "print(f'Metadata: {model_path}model_metadata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f04f9",
   "metadata": {},
   "source": [
    "## 8. Model Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== MODEL DEVELOPMENT SUMMARY ===')\n",
    "print(f'Best Model: {best_model_name}')\n",
    "print(f'Final Accuracy: {accuracy:.4f}')\n",
    "print(f'Training Samples: {len(X_train)}')\n",
    "print(f'Test Samples: {len(X_test)}')\n",
    "print(f'Number of Features: {X.shape[1]}')\n",
    "print(f'Number of Classes: {len(target_names)}')\n",
    "\n",
    "print('\\n=== NEXT STEPS ===')\n",
    "print('1. Deploy model to production API')\n",
    "print('2. Set up model monitoring and logging')\n",
    "print('3. Create model validation pipeline')\n",
    "print('4. Implement A/B testing framework')\n",
    "print('5. Schedule model retraining')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
