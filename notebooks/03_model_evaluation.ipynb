{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d08f510",
   "metadata": {},
   "source": [
    "# Risk Predictor - Model Evaluation and Monitoring\n",
    "\n",
    "This notebook focuses on comprehensive model evaluation, performance monitoring, and production readiness assessment.\n",
    "\n",
    "## Objectives:\n",
    "- Load and evaluate trained models\n",
    "- Performance analysis and validation\n",
    "- Model interpretability and explainability\n",
    "- Production monitoring setup\n",
    "- API integration testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944182ed",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Model interpretation\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print('SHAP not available. Install with: pip install shap')\n",
    "\n",
    "# API testing\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Utilities\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and components\n",
    "model_path = '/home/jovyan/data/models/'\n",
    "\n",
    "try:\n",
    "    # Load model components\n",
    "    model = joblib.load(f'{model_path}risk_predictor_model.pkl')\n",
    "    label_encoder = joblib.load(f'{model_path}label_encoder.pkl')\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(f'{model_path}model_metadata.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print('Model loaded successfully!')\n",
    "    print(f'Model type: {metadata[\"model_type\"]}')\n",
    "    print(f'Training accuracy: {metadata[\"accuracy\"]:.4f}')\n",
    "    print(f'Feature count: {len(metadata[\"feature_names\"])}')\n",
    "    print(f'Target classes: {metadata[\"target_classes\"]}')\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f'Model files not found: {e}')\n",
    "    print('Please run the model development notebook first to train and save the model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ce060",
   "metadata": {},
   "source": [
    "## 2. Load Test Data for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5553d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or load test dataset\n",
    "np.random.seed(123)  # Different seed for test data\n",
    "n_test = 1000\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 80, n_test),\n",
    "    'income': np.random.lognormal(10.5, 0.8, n_test),\n",
    "    'credit_score': np.random.normal(650, 100, n_test).clip(300, 850),\n",
    "    'employment_length': np.random.exponential(5, n_test).clip(0, 40),\n",
    "    'loan_amount': np.random.lognormal(10, 0.7, n_test),\n",
    "    'debt_to_income': np.random.beta(2, 5, n_test) * 100,\n",
    "    'previous_defaults': np.random.poisson(0.3, n_test),\n",
    "    'education_level': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "                                      n_test, p=[0.4, 0.35, 0.2, 0.05])\n",
    "})\n",
    "\n",
    "# Create true risk levels\n",
    "risk_score = (\n",
    "    -0.01 * test_df['age'] +\n",
    "    -0.00001 * test_df['income'] +\n",
    "    -0.005 * test_df['credit_score'] +\n",
    "    -0.02 * test_df['employment_length'] +\n",
    "    0.00002 * test_df['loan_amount'] +\n",
    "    0.02 * test_df['debt_to_income'] +\n",
    "    0.5 * test_df['previous_defaults'] +\n",
    "    np.random.normal(0, 0.5, n_test)\n",
    ")\n",
    "\n",
    "test_df['true_risk_level'] = pd.cut(risk_score, \n",
    "                                   bins=[-np.inf, -0.5, 0.5, np.inf], \n",
    "                                   labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "print(f'Test dataset created: {test_df.shape}')\n",
    "print(f'True risk distribution:\\n{test_df[\"true_risk_level\"].value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2cd298",
   "metadata": {},
   "source": [
    "## 3. Model Predictions and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca78ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "try:\n",
    "    X_test = test_df.drop('true_risk_level', axis=1)\n",
    "    y_true = label_encoder.transform(test_df['true_risk_level'])\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Convert back to labels for display\n",
    "    y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "    y_true_labels = test_df['true_risk_level']\n",
    "    \n",
    "    print('Predictions completed successfully!')\n",
    "    print(f'Predicted risk distribution:\\n{pd.Series(y_pred_labels).value_counts()}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Error making predictions: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d47eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print('=== MODEL PERFORMANCE ON TEST DATA ===')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n",
    "\n",
    "# Detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1eac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix visualization\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - Test Data')\n",
    "plt.xlabel('Predicted Risk Level')\n",
    "plt.ylabel('True Risk Level')\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "print('\\nPer-class Accuracy:')\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f'{class_name}: {class_accuracies[i]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e9186",
   "metadata": {},
   "source": [
    "## 4. ROC Curves and AUC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for each class (one-vs-rest)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "\n",
    "# Binarize the output\n",
    "n_classes = len(label_encoder.classes_)\n",
    "y_test_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'ROC curve of class {label_encoder.classes_[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print AUC scores\n",
    "print('AUC Scores by Class:')\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f'{class_name}: {roc_auc[i]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4bf915",
   "metadata": {},
   "source": [
    "## 5. Model Interpretability (SHAP Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29357bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis for model interpretability\n",
    "if SHAP_AVAILABLE:\n",
    "    try:\n",
    "        # Create SHAP explainer\n",
    "        explainer = shap.Explainer(model.predict, X_test.iloc[:100])  # Use subset for speed\n",
    "        shap_values = explainer(X_test.iloc[:100])\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, X_test.iloc[:100], show=False)\n",
    "        plt.title('SHAP Summary Plot - Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print('SHAP analysis completed successfully!')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'SHAP analysis failed: {e}')\n",
    "        print('This might be due to model type compatibility or data format issues.')\n",
    "else:\n",
    "    print('SHAP not available. Install with: pip install shap')\n",
    "    print('Alternative: Using basic feature importance from tree-based models')\n",
    "    \n",
    "    # Alternative: Feature importance for tree-based models\n",
    "    if hasattr(model.named_steps['classifier'], 'feature_importances_'):\n",
    "        importances = model.named_steps['classifier'].feature_importances_\n",
    "        feature_names = X_test.columns\n",
    "        \n",
    "        # Create feature importance plot\n",
    "        indices = np.argsort(importances)[::-1][:15]\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.bar(range(len(indices)), importances[indices])\n",
    "        plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=45)\n",
    "        plt.title('Feature Importance (Top 15)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984c29d",
   "metadata": {},
   "source": [
    "## 6. Model Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41cf8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show prediction examples with probabilities\n",
    "sample_indices = np.random.choice(len(X_test), 10, replace=False)\n",
    "\n",
    "print('=== PREDICTION EXAMPLES ===')\n",
    "print('Sample predictions with confidence scores:\\n')\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample = X_test.iloc[idx:idx+1]\n",
    "    pred_proba = model.predict_proba(sample)[0]\n",
    "    pred_class = model.predict(sample)[0]\n",
    "    pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "    true_label = y_true_labels.iloc[idx]\n",
    "    \n",
    "    print(f'Sample {idx}:')\n",
    "    print(f'  Age: {sample[\"age\"].iloc[0]}, Income: ${sample[\"income\"].iloc[0]:,.0f}')\n",
    "    print(f'  Credit Score: {sample[\"credit_score\"].iloc[0]:.0f}, Debt-to-Income: {sample[\"debt_to_income\"].iloc[0]:.1f}%')\n",
    "    print(f'  Predicted: {pred_label} (confidence: {max(pred_proba):.3f})')\n",
    "    print(f'  True: {true_label}')\n",
    "    print(f'  Class probabilities: {dict(zip(label_encoder.classes_, pred_proba))}')\n",
    "    print(f'  Correct: {\"✓\" if pred_label == true_label else \"✗\"}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50138298",
   "metadata": {},
   "source": [
    "## 7. API Integration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c38b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API endpoint (if available)\n",
    "API_BASE_URL = 'http://localhost:5001'  # Updated port\n",
    "\n",
    "def test_api_health():\n",
    "    \"\"\"Test if the API is running\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f'{API_BASE_URL}/api/health', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print('✓ API is running and healthy')\n",
    "            return True\n",
    "        else:\n",
    "            print(f'✗ API returned status code: {response.status_code}')\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'✗ API connection failed: {e}')\n",
    "        return False\n",
    "\n",
    "def test_prediction_endpoint(sample_data):\n",
    "    \"\"\"Test prediction endpoint with sample data\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f'{API_BASE_URL}/api/predict',\n",
    "            json=sample_data,\n",
    "            timeout=10\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print('✓ Prediction endpoint working')\n",
    "            print(f'  Response: {result}')\n",
    "            return True\n",
    "        else:\n",
    "            print(f'✗ Prediction failed with status: {response.status_code}')\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'✗ Prediction request failed: {e}')\n",
    "        return False\n",
    "\n",
    "# Run API tests\n",
    "print('=== API INTEGRATION TESTS ===')\n",
    "api_healthy = test_api_health()\n",
    "\n",
    "if api_healthy:\n",
    "    # Test with sample data\n",
    "    sample_request = {\n",
    "        'age': 35,\n",
    "        'income': 75000,\n",
    "        'credit_score': 720,\n",
    "        'employment_length': 8,\n",
    "        'loan_amount': 30000,\n",
    "        'debt_to_income': 25.5,\n",
    "        'previous_defaults': 0,\n",
    "        'education_level': 'Bachelor'\n",
    "    }\n",
    "    \n",
    "    test_prediction_endpoint(sample_request)\n",
    "else:\n",
    "    print('Skipping prediction tests - API not available')\n",
    "    print('Note: Make sure your Flask backend is running on port 5001')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda09318",
   "metadata": {},
   "source": [
    "## 8. Performance Monitoring Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create monitoring functions\n",
    "def log_prediction(input_data, prediction, confidence, timestamp=None):\n",
    "    \"\"\"Log prediction for monitoring\"\"\"\n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now().isoformat()\n",
    "    \n",
    "    log_entry = {\n",
    "        'timestamp': timestamp,\n",
    "        'input_data': input_data,\n",
    "        'prediction': prediction,\n",
    "        'confidence': confidence,\n",
    "        'model_version': metadata.get('model_type', 'unknown')\n",
    "    }\n",
    "    \n",
    "    return log_entry\n",
    "\n",
    "def calculate_model_drift(reference_data, new_data, threshold=0.05):\n",
    "    \"\"\"Simple data drift detection\"\"\"\n",
    "    drift_scores = {}\n",
    "    \n",
    "    for column in reference_data.select_dtypes(include=[np.number]).columns:\n",
    "        # Kolmogorov-Smirnov test\n",
    "        from scipy.stats import ks_2samp\n",
    "        statistic, p_value = ks_2samp(reference_data[column], new_data[column])\n",
    "        \n",
    "        drift_scores[column] = {\n",
    "            'ks_statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < threshold\n",
    "        }\n",
    "    \n",
    "    return drift_scores\n",
    "\n",
    "# Example usage\n",
    "print('=== MONITORING SETUP ===')\n",
    "print('Monitoring functions created:')\n",
    "print('- log_prediction(): For logging predictions')\n",
    "print('- calculate_model_drift(): For detecting data drift')\n",
    "\n",
    "# Example prediction logging\n",
    "sample_log = log_prediction(\n",
    "    input_data={'age': 30, 'income': 50000},\n",
    "    prediction='Medium',\n",
    "    confidence=0.85\n",
    ")\n",
    "print(f'\\nSample log entry: {sample_log}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb5f8cb",
   "metadata": {},
   "source": [
    "## 9. Model Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive model quality report\n",
    "def generate_model_report():\n",
    "    report = {\n",
    "        'model_info': {\n",
    "            'model_type': metadata.get('model_type', 'Unknown'),\n",
    "            'training_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'feature_count': len(metadata.get('feature_names', [])),\n",
    "            'target_classes': metadata.get('target_classes', []),\n",
    "        },\n",
    "        'performance_metrics': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'precision': float(precision),\n",
    "            'recall': float(recall),\n",
    "            'f1_score': float(f1),\n",
    "        },\n",
    "        'class_performance': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Add per-class performance\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        report['class_performance'][class_name] = {\n",
    "            'accuracy': float(class_accuracies[i]),\n",
    "            'auc_score': float(roc_auc.get(i, 0.0))\n",
    "        }\n",
    "    \n",
    "    # Add recommendations based on performance\n",
    "    if accuracy < 0.8:\n",
    "        report['recommendations'].append('Consider model retraining or feature engineering')\n",
    "    if min(class_accuracies) < 0.7:\n",
    "        worst_class = label_encoder.classes_[np.argmin(class_accuracies)]\n",
    "        report['recommendations'].append(f'Improve {worst_class} class prediction')\n",
    "    if max(roc_auc.values()) < 0.8:\n",
    "        report['recommendations'].append('Consider different algorithms or ensemble methods')\n",
    "    \n",
    "    if not report['recommendations']:\n",
    "        report['recommendations'].append('Model performance is satisfactory')\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and display report\n",
    "model_report = generate_model_report()\n",
    "\n",
    "print('=== MODEL QUALITY REPORT ===')\n",
    "print(json.dumps(model_report, indent=2))\n",
    "\n",
    "# Save report\n",
    "report_path = '/home/jovyan/data/models/model_quality_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(model_report, f, indent=2)\n",
    "\n",
    "print(f'\\nReport saved to: {report_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9cf25",
   "metadata": {},
   "source": [
    "## 10. Production Readiness Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production readiness checklist\n",
    "checklist = {\n",
    "    'Model Performance': {\n",
    "        'Accuracy > 80%': accuracy > 0.8,\n",
    "        'All classes AUC > 70%': min(roc_auc.values()) > 0.7,\n",
    "        'Balanced class performance': (max(class_accuracies) - min(class_accuracies)) < 0.3\n",
    "    },\n",
    "    'Technical Requirements': {\n",
    "        'Model serialized': True,  # We saved the model\n",
    "        'Preprocessing pipeline included': True,  # Part of the model\n",
    "        'Label encoder available': True,  # We saved it\n",
    "        'Metadata documented': True,  # We have metadata\n",
    "    },\n",
    "    'API Integration': {\n",
    "        'API endpoint accessible': api_healthy if 'api_healthy' in locals() else False,\n",
    "        'Prediction format standardized': True,  # JSON format\n",
    "        'Error handling implemented': False,  # Need to implement\n",
    "    },\n",
    "    'Monitoring & Logging': {\n",
    "        'Prediction logging ready': True,  # Functions created\n",
    "        'Performance monitoring setup': True,  # Functions created\n",
    "        'Data drift detection ready': True,  # Function created\n",
    "        'Alerting system configured': False,  # Need to implement\n",
    "    }\n",
    "}\n",
    "\n",
    "print('=== PRODUCTION READINESS CHECKLIST ===')\n",
    "print()\n",
    "\n",
    "overall_ready = True\n",
    "for category, checks in checklist.items():\n",
    "    print(f'{category}:')\n",
    "    category_ready = True\n",
    "    for check, status in checks.items():\n",
    "        status_icon = '✓' if status else '✗'\n",
    "        print(f'  {status_icon} {check}')\n",
    "        if not status:\n",
    "            category_ready = False\n",
    "            overall_ready = False\n",
    "    print(f'  Category status: {\"READY\" if category_ready else \"NEEDS WORK\"}')\n",
    "    print()\n",
    "\n",
    "print(f'Overall Production Readiness: {\"READY\" if overall_ready else \"NEEDS WORK\"}')\n",
    "\n",
    "if not overall_ready:\n",
    "    print('\\nRecommended next steps:')\n",
    "    print('- Implement proper error handling in API')\n",
    "    print('- Set up alerting system for monitoring')\n",
    "    print('- Create automated model validation pipeline')\n",
    "    print('- Implement model versioning strategy')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
