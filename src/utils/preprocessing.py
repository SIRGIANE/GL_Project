import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
import joblib
from typing import Tuple, Optional
import warnings
warnings.filterwarnings('ignore')


class BugPreprocessor:
    """Classe principale pour le pr√©traitement des donn√©es de bugs"""
    
    def __init__(self, config: Optional[dict] = None):
        """
        Initialise le pr√©traiteur avec configuration
        
        Args:
            config: Configuration du pr√©traitement
        """
        self.config = config or {
            'test_size': 0.2,
            'random_state': 42,
            'smote_strategy': 'auto',
            'n_pca_components': None,  # None pour auto-selection
            'scale_features': True,
            'apply_smote': True,
            'apply_pca': True
        }
        
        self.scaler = None
        self.pca = None
        self.label_encoder = None
        self.feature_names = None
        
    def apply_smote(self, X: pd.DataFrame, Y: pd.DataFrame, 
                   random_state: int = 42, 
                   sampling_strategy: str = 'auto') -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Applique SMOTE pour √©quilibrer le dataset
        
        Args:
            X: Features
            Y: Target
            random_state: Seed pour reproductibilit√©
            sampling_strategy: Strat√©gie d'oversampling
            
        Returns:
            X_res, Y_res √©quilibr√©s
        """
        print("üîÑ Application de SMOTE...")
        
        sm = SMOTE(random_state=random_state, sampling_strategy=sampling_strategy)
        X_res, Y_res = sm.fit_resample(X, Y)
        
        Y_res = pd.DataFrame(Y_res, columns=Y.columns)
        X_res = pd.DataFrame(X_res, columns=X.columns)
        
        print(f"   ‚úì Avant SMOTE: {X.shape[0]} √©chantillons")
        print(f"   ‚úì Apr√®s SMOTE: {X_res.shape[0]} √©chantillons")
        print(f"   ‚úì Distribution: {Y_res.iloc[:, 0].value_counts().to_dict()}")
        
        return X_res, Y_res
    
    def encode_labels(self, Y_df: pd.DataFrame) -> Tuple[np.ndarray, LabelEncoder]:
        """
        Encode les labels cibles
        
        Args:
            Y_df: DataFrame des labels
            
        Returns:
            y_encoded, label_encoder
        """
        print("üî§ Encodage des labels...")
        
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(Y_df.values.ravel())
        
        unique_labels = np.unique(y_encoded)
        counts = np.bincount(y_encoded)
        
        print(f"   ‚úì Classes: {unique_labels}")
        print(f"   ‚úì Distribution: {dict(zip(unique_labels, counts))}")
        
        return y_encoded, self.label_encoder
    
    def split_data(self, X_arr: np.ndarray, y_arr: np.ndarray, 
                  test_size: float = 0.2, random_state: int = 42) -> Tuple:
        """
        Split les donn√©es en train/test
        
        Args:
            X_arr: Features array
            y_arr: Labels array
            test_size: Proportion test
            random_state: Seed
            
        Returns:
            X_train, X_test, y_train, y_test
        """
        print(f"‚úÇÔ∏è  Split des donn√©es (test_size={test_size})...")
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_arr, y_arr, 
            test_size=test_size, 
            random_state=random_state,
            stratify=y_arr
        )
        
        print(f"   ‚úì Train: {X_train.shape[0]} √©chantillons")
        print(f"   ‚úì Test: {X_test.shape[0]} √©chantillons")
        print(f"   ‚úì Distribution train: {np.bincount(y_train)}")
        print(f"   ‚úì Distribution test: {np.bincount(y_test)}")
        
        return X_train, X_test, y_train, y_test
    
    def scale_features(self, X_train: np.ndarray, X_test: np.ndarray) -> Tuple:
        """
        Normalise les features
        
        Args:
            X_train: Train features
            X_test: Test features
            
        Returns:
            X_train_scaled, X_test_scaled, scaler
        """
        print("üìä Normalisation des features...")
        
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        print(f"   ‚úì Train scaled: {X_train_scaled.shape}")
        print(f"   ‚úì Test scaled: {X_test_scaled.shape}")
        
        return X_train_scaled, X_test_scaled, self.scaler
    
    def apply_pca(self, X_train: np.ndarray, X_test: np.ndarray, 
                 n_components: Optional[int] = None, variance_threshold: float = 0.95) -> Tuple:
        """
        Applique PCA pour r√©duction de dimension
        
        Args:
            X_train: Train features
            X_test: Test features
            n_components: Nombre de composants (None pour auto)
            variance_threshold: Seuil de variance expliqu√©e
            
        Returns:
            X_train_pca, X_test_pca, pca
        """
        print("üîç Application de PCA...")
        
        if n_components is None:
            # PCA avec variance expliqu√©e
            pca = PCA(n_components=variance_threshold)
            X_train_pca = pca.fit_transform(X_train)
            n_components = pca.n_components_
        else:
            # PCA avec nombre fixe de composants
            pca = PCA(n_components=n_components)
            X_train_pca = pca.fit_transform(X_train)
        
        X_test_pca = pca.transform(X_test)
        
        explained_variance = pca.explained_variance_ratio_.sum()
        
        print(f"   ‚úì R√©duction: {X_train.shape[1]} ‚Üí {n_components} composants")
        print(f"   ‚úì Variance expliqu√©e: {explained_variance:.2%}")
        print(f"   ‚úì Composants importants: {pca.explained_variance_ratio_[:5].round(3)}...")
        
        self.pca = pca
        return X_train_pca, X_test_pca, pca
    
    def save_preprocessors(self, path: str = 'models/'):
        """
        Sauvegarde les pr√©processeurs
        
        Args:
            path: Chemin de sauvegarde
        """
        import os
        os.makedirs(path, exist_ok=True)
        
        if self.scaler:
            joblib.dump(self.scaler, f'{path}scaler.pkl')
        if self.pca:
            joblib.dump(self.pca, f'{path}pca.pkl')
        if self.label_encoder:
            joblib.dump(self.label_encoder, f'{path}label_encoder.pkl')
        
        # Sauvegarder la configuration
        config_data = {
            'config': self.config,
            'feature_names': self.feature_names,
            'saved_at': pd.Timestamp.now().isoformat()
        }
        joblib.dump(config_data, f'{path}preprocessor_config.pkl')
        
        print(f"üíæ Pr√©processeurs sauvegard√©s dans {path}")
    
    def load_preprocessors(self, path: str = 'models/') -> Tuple:
        """
        Charge les pr√©processeurs
        
        Args:
            path: Chemin des pr√©processeurs
            
        Returns:
            scaler, pca, label_encoder
        """
        try:
            self.scaler = joblib.load(f'{path}scaler.pkl')
            self.pca = joblib.load(f'{path}pca.pkl')
            self.label_encoder = joblib.load(f'{path}label_encoder.pkl')
            
            # Charger la configuration
            config_data = joblib.load(f'{path}preprocessor_config.pkl')
            self.config = config_data.get('config', {})
            self.feature_names = config_data.get('feature_names', [])
            
            print(f"üìÇ Pr√©processeurs charg√©s depuis {path}")
            print(f"   Configuration: {self.config}")
            
            return self.scaler, self.pca, self.label_encoder
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur chargement pr√©processeurs: {e}")
            return None, None, None
    
    def full_preprocessing_pipeline(self, X: pd.DataFrame, Y: pd.DataFrame) -> dict:
        """
        Pipeline complet de pr√©traitement
        
        Args:
            X: Features
            Y: Target
            
        Returns:
            Dictionnaire avec toutes les donn√©es pr√©trait√©es
        """
        print("=" * 60)
        print("üöÄ D√âMARRAGE DU PIPELINE DE PR√âTRAITEMENT")
        print("=" * 60)
        
        # Sauvegarder les noms des features
        self.feature_names = X.columns.tolist()
        print(f"üìã Features: {len(self.feature_names)}")
        
        results = {
            'original_X': X,
            'original_Y': Y,
            'feature_names': self.feature_names
        }
        
        # 1. SMOTE (optionnel)
        if self.config.get('apply_smote', True):
            X_res, Y_res = self.apply_smote(
                X, Y,
                random_state=self.config.get('random_state', 42),
                sampling_strategy=self.config.get('smote_strategy', 'auto')
            )
        else:
            X_res, Y_res = X, Y
            print("‚ö†Ô∏è SMOTE d√©sactiv√©")
        
        results['X_resampled'] = X_res
        results['Y_resampled'] = Y_res
        
        # 2. Encodage des labels
        y_encoded, label_encoder = self.encode_labels(Y_res)
        results['label_encoder'] = label_encoder
        
        # 3. Split des donn√©es
        X_train, X_test, y_train, y_test = self.split_data(
            X_res.values, y_encoded,
            test_size=self.config.get('test_size', 0.2),
            random_state=self.config.get('random_state', 42)
        )
        
        results.update({
            'X_train_raw': X_train,
            'X_test_raw': X_test,
            'y_train': y_train,
            'y_test': y_test
        })
        
        # 4. Normalisation (optionnel)
        if self.config.get('scale_features', True):
            X_train_scaled, X_test_scaled, scaler = self.scale_features(X_train, X_test)
            results['scaler'] = scaler
        else:
            X_train_scaled, X_test_scaled = X_train, X_test
            print("‚ö†Ô∏è Normalisation d√©sactiv√©e")
        
        results.update({
            'X_train_scaled': X_train_scaled,
            'X_test_scaled': X_test_scaled
        })
        
        # 5. PCA (optionnel)
        if self.config.get('apply_pca', True):
            n_components = self.config.get('n_pca_components')
            X_train_pca, X_test_pca, pca = self.apply_pca(
                X_train_scaled, X_test_scaled,
                n_components=n_components
            )
            results['pca'] = pca
            X_train_final, X_test_final = X_train_pca, X_test_pca
        else:
            X_train_final, X_test_final = X_train_scaled, X_test_scaled
            print("‚ö†Ô∏è PCA d√©sactiv√©")
        
        results.update({
            'X_train_final': X_train_final,
            'X_test_final': X_test_final
        })
        
        # 6. Sauvegarde
        if self.config.get('save_preprocessors', True):
            self.save_preprocessors()
        
        print("\n" + "=" * 60)
        print("‚úÖ PR√âTRAITEMENT TERMIN√â AVEC SUCC√àS!")
        print("=" * 60)
        
        summary = {
            'samples_total': len(X),
            'samples_after_smote': len(X_res),
            'train_samples': len(X_train_final),
            'test_samples': len(X_test_final),
            'original_features': X.shape[1],
            'final_features': X_train_final.shape[1],
            'feature_reduction': f"{X.shape[1] - X_train_final.shape[1]} features",
            'class_distribution': {
                'train': np.bincount(y_train).tolist(),
                'test': np.bincount(y_test).tolist()
            }
        }
        
        print("\nüìä R√âSUM√â:")
        for key, value in summary.items():
            print(f"   ‚Ä¢ {key}: {value}")
        
        return results
    
    def transform_new_data(self, X_new: pd.DataFrame) -> np.ndarray:
        """
        Transforme de nouvelles donn√©es avec les pr√©processeurs entra√Æn√©s
        
        Args:
            X_new: Nouvelles donn√©es
            
        Returns:
            Donn√©es transform√©es
        """
        if self.scaler is None or self.pca is None:
            raise ValueError("Pr√©processeurs non charg√©s. Appelez load_preprocessors() d'abord.")
        
        print(f"üîÑ Transformation de {len(X_new)} nouveaux √©chantillons...")
        
        # V√©rifier les features
        if self.feature_names and set(X_new.columns) != set(self.feature_names):
            print(f"‚ö†Ô∏è Features diff√©rentes. Attendu: {self.feature_names[:5]}...")
            # R√©organiser les colonnes si n√©cessaire
            missing = set(self.feature_names) - set(X_new.columns)
            extra = set(X_new.columns) - set(self.feature_names)
            if missing:
                print(f"   ‚ùå Features manquantes: {list(missing)[:5]}...")
                raise ValueError(f"Features manquantes: {list(missing)}")
        
        # Transformation
        X_scaled = self.scaler.transform(X_new.values)
        X_pca = self.pca.transform(X_scaled)
        
        print(f"   ‚úì Shape finale: {X_pca.shape}")
        return X_pca


# Fonctions utilitaires (compatibilit√© avec ancien code)
def apply_smote(X, Y, random_state=12, sampling_strategy=1.0):
    """Fonction wrapper pour compatibilit√©"""
    preprocessor = BugPreprocessor()
    X_res, Y_res = preprocessor.apply_smote(X, Y, random_state, sampling_strategy)
    return X_res, Y_res

def encode_labels(Y_df):
    """Fonction wrapper pour compatibilit√©"""
    preprocessor = BugPreprocessor()
    y_encoded, label_encoder = preprocessor.encode_labels(Y_df)
    return y_encoded, label_encoder

def split_data(X_arr, y_arr, test_size=0.25, random_state=0):
    """Fonction wrapper pour compatibilit√©"""
    preprocessor = BugPreprocessor({'test_size': test_size, 'random_state': random_state})
    return preprocessor.split_data(X_arr, y_arr, test_size, random_state)

def scale_features(X_train, X_test):
    """Fonction wrapper pour compatibilit√©"""
    preprocessor = BugPreprocessor()
    return preprocessor.scale_features(X_train, X_test)

def apply_pca(X_train, X_test, n_components=6):
    """Fonction wrapper pour compatibilit√©"""
    preprocessor = BugPreprocessor({'n_pca_components': n_components})
    return preprocessor.apply_pca(X_train, X_test, n_components)

def save_preprocessors(scaler, pca, path='models/'):
    """Fonction wrapper pour compatibilit√©"""
    preprocessor = BugPreprocessor()
    preprocessor.scaler = scaler
    preprocessor.pca = pca
    preprocessor.save_preprocessors(path)

def load_preprocessors(path='models/'):
    """Fonction wrapper pour compatibilit√©"""
    preprocessor = BugPreprocessor()
    scaler, pca, _ = preprocessor.load_preprocessors(path)
    return scaler, pca


if __name__ == '__main__':
    # Test du pipeline
    print("üß™ Test du pr√©traitement...")
    
    # Cr√©er des donn√©es de test
    np.random.seed(42)
    n_samples = 1000
    n_features = 20
    
    X_test = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    
    # Cr√©er des labels d√©s√©quilibr√©s
    y_test = np.random.choice([0, 1], n_samples, p=[0.9, 0.1])
    Y_test = pd.DataFrame(y_test, columns=['target'])
    
    print(f"Donn√©es de test: {X_test.shape}")
    print(f"Distribution originale: {pd.Series(y_test).value_counts().to_dict()}")
    
    # Utiliser le pipeline complet
    preprocessor = BugPreprocessor({
        'test_size': 0.2,
        'random_state': 42,
        'apply_smote': True,
        'apply_pca': True,
        'n_pca_components': 10,
        'save_preprocessors': False
    })
    
    results = preprocessor.full_preprocessing_pipeline(X_test, Y_test)
    
    print("\n‚úÖ Test r√©ussi!")