
"""
Pipeline d'entra√Ænement des mod√®les de pr√©diction de bugs
"""

import sys
import os
import joblib
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Any

# Add the project root to the system path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from src.core.dataset import load_and_preprocess_data
from src.utils.preprocessing import (
    apply_smote, encode_labels, split_data,
    scale_features, apply_pca, save_preprocessors
)

# Import des mod√®les depuis la nouvelle architecture
from src.core.model_factory import ModelFactory
from src.core.model import BaseModel

# Define paths
MODELS_DIR = 'bug-predictor/models/'
RESULTS_DIR = 'bug-predictor/results/'
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

def train_models_selected(
    data_path: str = 'GL_Project/data/',
    models_to_train: List[str] = None,
    use_smote: bool = True,
    use_pca: bool = True,
    test_size: float = 0.2,
    random_state: int = 42
) -> Tuple[Dict[str, BaseModel], Dict[str, Any]]:
    """
    Entra√Æne les mod√®les s√©lectionn√©s
    
    Args:
        data_path: Chemin vers les donn√©es
        models_to_train: Liste des mod√®les √† entra√Æner
        use_smote: Appliquer SMOTE pour √©quilibrer les classes
        use_pca: Appliquer PCA pour r√©duire la dimension
        test_size: Proportion des donn√©es de test
        random_state: Seed pour la reproductibilit√©
        
    Returns:
        Tuple contenant:
            - Dictionnaire des mod√®les entra√Æn√©s
            - Dictionnaire des m√©triques et pr√©processeurs
    """
    print("üöÄ D√©marrage du pipeline d'entra√Ænement...")
    print(f"üìã Mod√®les √† entra√Æner: {models_to_train}")
    
    # Liste par d√©faut des mod√®les si non sp√©cifi√©e
    if models_to_train is None:
        models_to_train = [
            'LogisticRegressionModel',
            'LSTMModel',
            'RandomForestModel',
            'SVMModel',
            'KNNModel',
            'NaiveBayesModel',
            'DecisionTreeModel'
        ]
    
    # 1. Chargement et pr√©traitement des donn√©es
    print("\nüì• 1. Chargement des donn√©es...")
    X_df, Y_df, _ = load_and_preprocess_data(data_path=data_path)
    print(f"   ‚úì Donn√©es charg√©es: {X_df.shape[0]} √©chantillons, {X_df.shape[1]} features")
    
    # 2. Application de SMOTE (optionnel)
    if use_smote:
        print("\n‚öñÔ∏è  2. Application de SMOTE pour √©quilibrer les classes...")
        X_res, Y_res = apply_smote(X_df, Y_df)
        print(f"   ‚úì Donn√©es apr√®s SMOTE: {X_res.shape[0]} √©chantillons")
    else:
        X_res, Y_res = X_df, Y_df
        print("\n‚öñÔ∏è  2. SMOTE d√©sactiv√©")
    
    # 3. Encodage des labels
    print("\nüî§ 3. Encodage des labels...")
    y_encoded, label_encoder = encode_labels(Y_res)
    class_distribution = np.bincount(y_encoded)
    print(f"   ‚úì Classe 0: {class_distribution[0]} √©chantillons")
    print(f"   ‚úì Classe 1: {class_distribution[1]} √©chantillons")
    
    # 4. Split des donn√©es
    print(f"\n‚úÇÔ∏è  4. Split des donn√©es (test_size={test_size})...")
    X_train, X_test, y_train, y_test = split_data(
        X_res.values, y_encoded, test_size=test_size, random_state=random_state
    )
    print(f"   ‚úì Train: {X_train.shape[0]} √©chantillons")
    print(f"   ‚úì Test: {X_test.shape[0]} √©chantillons")
    
    # 5. Normalisation des features
    print("\nüìä 5. Normalisation des features...")
    X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)
    print("   ‚úì Features normalis√©es")
    
    # 6. Application de PCA (optionnel)
    if use_pca:
        print("\nüîç 6. Application de PCA...")
        X_train_pca, X_test_pca, pca = apply_pca(X_train_scaled, X_test_scaled)
        print(f"   ‚úì R√©duction de dimension: {X_train.shape[1]} ‚Üí {X_train_pca.shape[1]} features")
        X_train_final, X_test_final = X_train_pca, X_test_pca
    else:
        X_train_final, X_test_final = X_train_scaled, X_test_scaled
        pca = None
        print("\nüîç 6. PCA d√©sactiv√©")
    
    # Sauvegarde des pr√©processeurs
    print("\nüíæ 7. Sauvegarde des pr√©processeurs...")
    save_preprocessors(scaler, pca, path=MODELS_DIR)
    print("   ‚úì Pr√©processeurs sauvegard√©s")
    
    # Initialisation des r√©sultats
    trained_models = {}
    all_metrics = {}
    training_summary = []
    
    print(f"\nü§ñ 8. Entra√Ænement des mod√®les ({len(models_to_train)} mod√®les)...")
    
    # Param√®tres par d√©faut pour chaque mod√®le
    default_params = {
        'LogisticRegressionModel': {
            'C': 1.0,
            'max_iter': 1000,
            'calibrate': True,
            'random_state': random_state
        },
        'LSTMModel': {
            'units': 100,
            'dropout_rate': 0.2,
            'input_dim': X_train_final.shape[1]
        },
        'RandomForestModel': {
            'n_estimators': 100,
            'max_depth': 10,
            'random_state': random_state
        },
        'SVMModel': {
            'kernel': 'rbf',
            'C': 1.0,
            'probability': True,
            'random_state': random_state
        },
        'KNNModel': {
            'n_neighbors': 5,
            'weights': 'uniform'
        },
        'NaiveBayesModel': {
            'var_smoothing': 1e-9
        },
        'DecisionTreeModel': {
            'max_depth': None,
            'random_state': random_state
        }
    }
    
    # Entra√Ænement de chaque mod√®le
    for model_name in models_to_train:
        try:
            print(f"\n   üîß {model_name}...")
            
            # Cr√©ation du mod√®le avec les param√®tres par d√©faut
            params = default_params.get(model_name, {})
            model = ModelFactory.create_model(model_name.lower().replace('model', ''), **params)
            
            # D√©finir les noms des features
            if hasattr(model, 'set_feature_names'):
                feature_names = [f'feature_{i}' for i in range(X_train_final.shape[1])]
                model.set_feature_names(feature_names)
            
            # Entra√Ænement sp√©cifique pour LSTM
            if model_name == 'LSTMModel':
                model.train(X_train_final, y_train, epochs=50, batch_size=32)
            else:
                model.train(X_train_final, y_train)
            
            # √âvaluation
            metrics = model.evaluate(X_test_final, y_test)
            
            # Stockage des r√©sultats
            trained_models[model_name] = model
            all_metrics[model_name] = metrics
            
            # Sauvegarde du mod√®le
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_filename = f"{MODELS_DIR}{model_name}_{timestamp}.joblib"
            
            # Gestion sp√©ciale pour les mod√®les Keras
            if model_name in ['LSTMModel']:
                keras_filename = f"{MODELS_DIR}{model_name}_{timestamp}.h5"
                model.model.save(keras_filename)
                print(f"     üíæ Mod√®le Keras sauvegard√©: {keras_filename}")
            
            model.save(model_filename)
            print(f"     üíæ Mod√®le sauvegard√©: {model_filename}")
            
            # Ajout au r√©sum√©
            training_summary.append({
                'Model': model_name,
                'Accuracy': f"{metrics['accuracy']:.4f}",
                'Precision': f"{metrics['precision']:.4f}",
                'Recall': f"{metrics['recall']:.4f}",
                'F1-Score': f"{metrics['f1_score']:.4f}",
                'ROC-AUC': f"{metrics.get('roc_auc', 0):.4f}",
                'Training Time (s)': f"{model.training_time:.2f}"
            })
            
            print(f"     ‚úÖ Performance: Accuracy={metrics['accuracy']:.3f}, F1={metrics['f1_score']:.3f}")
            
        except Exception as e:
            print(f"     ‚ùå Erreur avec {model_name}: {str(e)}")
            continue
    
    # 9. Analyse comparative
    print("\nüìä 9. Analyse comparative des mod√®les...")
    
    if training_summary:
        # Trier par F1-Score
        training_summary.sort(key=lambda x: float(x['F1-Score']), reverse=True)
        
        print("\nüèÜ CLASSEMENT DES MOD√àLES (par F1-Score):")
        print("-" * 80)
        for i, model_info in enumerate(training_summary):
            rank = "ü•á" if i == 0 else "ü•à" if i == 1 else "ü•â" if i == 2 else f"{i+1:2d}."
            print(f"{rank} {model_info['Model']:25s}")
            print(f"   F1-Score: {model_info['F1-Score']} | Accuracy: {model_info['Accuracy']} | ROC-AUC: {model_info['ROC-AUC']}")
            print(f"   Training Time: {model_info['Training Time (s)']}s")
            print()
    
    # 10. Sauvegarde des r√©sultats
    print("\nüíæ 10. Sauvegarde des r√©sultats...")
    
    # Sauvegarde des m√©triques
    import pandas as pd
    import json
    
    metrics_df = pd.DataFrame(training_summary)
    metrics_csv = f"{RESULTS_DIR}model_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    metrics_df.to_csv(metrics_csv, index=False)
    print(f"   ‚úì M√©triques sauvegard√©es: {metrics_csv}")
    
    # Sauvegarde des m√©triques d√©taill√©es en JSON
    metrics_json = f"{RESULTS_DIR}detailed_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(metrics_json, 'w') as f:
        json.dump(all_metrics, f, indent=2)
    print(f"   ‚úì M√©triques d√©taill√©es sauvegard√©es: {metrics_json}")
    
    # Sauvegarde du r√©sum√© d'entra√Ænement
    summary = {
        'timestamp': datetime.now().isoformat(),
        'models_trained': models_to_train,
        'dataset_info': {
            'original_samples': X_df.shape[0],
            'final_train_samples': X_train_final.shape[0],
            'test_samples': X_test_final.shape[0],
            'n_features_original': X_df.shape[1],
            'n_features_final': X_train_final.shape[1],
            'use_smote': use_smote,
            'use_pca': use_pca,
            'test_size': test_size,
            'random_state': random_state
        },
        'best_model': training_summary[0]['Model'] if training_summary else None,
        'best_f1_score': training_summary[0]['F1-Score'] if training_summary else None
    }
    
    summary_json = f"{RESULTS_DIR}training_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(summary_json, 'w') as f:
        json.dump(summary, f, indent=2)
    print(f"   ‚úì R√©sum√© d'entra√Ænement sauvegard√©: {summary_json}")
    
    # 11. Retour des r√©sultats
    results = {
        'trained_models': trained_models,
        'metrics': all_metrics,
        'training_summary': training_summary,
        'preprocessors': {
            'scaler': scaler,
            'pca': pca,
            'label_encoder': label_encoder
        },
        'data': {
            'X_train': X_train_final,
            'X_test': X_test_final,
            'y_train': y_train,
            'y_test': y_test
        }
    }
    
    print("\n" + "=" * 60)
    print("‚úÖ PIPELINE D'ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS!")
    print("=" * 60)
    
    print(f"\nüìä R√âSUM√â FINAL:")
    print(f"   ‚Ä¢ Mod√®les entra√Æn√©s: {len(trained_models)}/{len(models_to_train)}")
    print(f"   ‚Ä¢ Meilleur mod√®le: {summary['best_model']}")
    print(f"   ‚Ä¢ Meilleur F1-Score: {summary['best_f1_score']}")
    print(f"   ‚Ä¢ Donn√©es d'entra√Ænement: {X_train_final.shape}")
    print(f"   ‚Ä¢ Donn√©es de test: {X_test_final.shape}")
    
    return trained_models, results

def train_single_model(
    model_type: str,
    data_path: str = 'bug-predictor/data/',
    model_params: Dict[str, Any] = None,
    **training_kwargs
) -> Tuple[BaseModel, Dict[str, Any]]:
    """
    Entra√Æne un seul mod√®le sp√©cifique
    
    Args:
        model_type: Type de mod√®le √† entra√Æner
        data_path: Chemin vers les donn√©es
        model_params: Param√®tres sp√©cifiques du mod√®le
        **training_kwargs: Arguments d'entra√Ænement suppl√©mentaires
        
    Returns:
        Tuple (mod√®le entra√Æn√©, m√©triques)
    """
    print(f"üöÄ Entra√Ænement du mod√®le {model_type}...")
    
    # Chargement des donn√©es (r√©utilise la logique de train_models_selected)
    X_df, Y_df, _ = load_and_preprocess_data(data_path=data_path)
    X_res, Y_res = apply_smote(X_df, Y_df)
    y_encoded, label_encoder = encode_labels(Y_res)
    X_train, X_test, y_train, y_test = split_data(
        X_res.values, y_encoded, test_size=0.2, random_state=42
    )
    X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)
    X_train_pca, X_test_pca, pca = apply_pca(X_train_scaled, X_test_scaled)
    
    # Cr√©ation du mod√®le
    if model_params is None:
        model_params = {}
    
    model = ModelFactory.create_model(model_type, **model_params)
    
    # D√©finir les noms des features
    if hasattr(model, 'set_feature_names'):
        feature_names = [f'feature_{i}' for i in range(X_train_pca.shape[1])]
        model.set_feature_names(feature_names)
    
    # Entra√Ænement
    if model_type == 'lstm':
        model.train(X_train_pca, y_train, **training_kwargs)
    else:
        model.train(X_train_pca, y_train, **training_kwargs)
    
    # √âvaluation
    metrics = model.evaluate(X_test_pca, y_test)
    
    # Sauvegarde
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_filename = f"{MODELS_DIR}{model_type}_{timestamp}.joblib"
    model.save(model_filename)
    
    print(f"‚úÖ {model_type} entra√Æn√© avec succ√®s!")
    print(f"üìä Performance: Accuracy={metrics['accuracy']:.3f}, F1={metrics['f1_score']:.3f}")
    print(f"üíæ Mod√®le sauvegard√©: {model_filename}")
    
    return model, metrics

def load_trained_model(model_path: str) -> BaseModel:
    """
    Charge un mod√®le entra√Æn√© pr√©c√©demment
    
    Args:
        model_path: Chemin vers le fichier du mod√®le
        
    Returns:
        Mod√®le charg√©
    """
    print(f"üìÇ Chargement du mod√®le: {model_path}")
    
    model = BaseModel("", {})
    model.load(model_path)
    
    print(f"‚úÖ Mod√®le charg√©: {model.model_name}")
    print(f"   Entra√Æn√© le: {model.training_time}")
    print(f"   M√©triques: {model.metrics}")
    
    return model

def compare_models(
    model_paths: List[str],
    X_test: np.ndarray,
    y_test: np.ndarray
) -> Dict[str, Dict[str, float]]:
    """
    Compare plusieurs mod√®les pr√©-entra√Æn√©s
    
    Args:
        model_paths: Liste des chemins vers les mod√®les
        X_test: Donn√©es de test
        y_test: Labels de test
        
    Returns:
        Dictionnaire des m√©triques pour chaque mod√®le
    """
    print("üìä Comparaison de mod√®les...")
    
    comparison_results = {}
    
    for model_path in model_paths:
        try:
            # Charger le mod√®le
            model = load_trained_model(model_path)
            
            # √âvaluer
            metrics = model.evaluate(X_test, y_test)
            
            comparison_results[model.model_name] = {
                'accuracy': metrics['accuracy'],
                'f1_score': metrics['f1_score'],
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'roc_auc': metrics.get('roc_auc', 0),
                'training_time': model.training_time
            }
            
            print(f"   ‚úì {model.model_name}: F1={metrics['f1_score']:.3f}")
            
        except Exception as e:
            print(f"   ‚ùå Erreur avec {model_path}: {e}")
            continue
    
    # Trier par F1-Score
    sorted_results = dict(sorted(
        comparison_results.items(),
        key=lambda x: x[1]['f1_score'],
        reverse=True
    ))
    
    print("\nüèÜ CLASSEMENT:")
    for i, (model_name, metrics) in enumerate(sorted_results.items()):
        rank = "ü•á" if i == 0 else "ü•à" if i == 1 else "ü•â" if i == 2 else f"{i+1:2d}."
        print(f"{rank} {model_name:20s} | F1: {metrics['f1_score']:.3f} | Acc: {metrics['accuracy']:.3f}")
    
    return sorted_results

if __name__ == '__main__':
    """
    Exemple d'utilisation
    """
    
    # Option 1: Entra√Æner tous les mod√®les par d√©faut
    trained_models, results = train_models_selected()
    
    # Option 2: Entra√Æner seulement certains mod√®les
    # selected_models = ['RandomForestModel', 'LogisticRegressionModel', 'SVMModel']
    # trained_models, results = train_models_selected(models_to_train=selected_models)
    
    # Option 3: Entra√Æner un seul mod√®le
    # model, metrics = train_single_model(
    #     model_type='random_forest',
    #     model_params={'n_estimators': 200, 'max_depth': 15},
    #     data_path='bug-predictor/data/'
    # )
    
    # Option 4: Comparer des mod√®les existants
    # model_files = [
    #     'bug-predictor/models/RandomForestModel_20250101_120000.joblib',
    #     'bug-predictor/models/LogisticRegressionModel_20250101_120000.joblib'
    # ]
    # comparison = compare_models(model_files, results['data']['X_test'], results['data']['y_test'])