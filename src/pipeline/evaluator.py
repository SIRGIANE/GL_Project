
"""
√âvaluateur de mod√®les de pr√©diction de bugs
√âvalue les mod√®les sauvegard√©s sur un jeu de test coh√©rent
"""

import sys
import os
import joblib
import numpy as np
import pandas as pd
import json
from datetime import datetime
from typing import Dict, List, Tuple, Any
import matplotlib.pyplot as plt
import seaborn as sns

# Add the project root to the system path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from src.utils.preprocessing import load_preprocessors
from src.core.dataset import load_and_preprocess_data
from src.utils.preprocessing import (
    apply_smote, encode_labels, split_data,
    scale_features, apply_pca
)
from src.core.model import BaseModel
from src.utils.metrics import evaluate_model_performance

# Define paths
MODELS_DIR = 'bug-predictor/models/'
RESULTS_DIR = 'bug-predictor/results/'
EVALUATION_DIR = 'bug-predictor/evaluation/'
os.makedirs(EVALUATION_DIR, exist_ok=True)

class ModelEvaluator:
    """Classe pour √©valuer les mod√®les de pr√©diction de bugs"""
    
    def __init__(self, models_dir: str = MODELS_DIR):
        """
        Initialise l'√©valuateur
        
        Args:
            models_dir: R√©pertoire contenant les mod√®les
        """
        self.models_dir = models_dir
        self.loaded_models = {}
        self.evaluation_results = {}
        self.test_data = None
        
    def prepare_test_data(self, 
                         data_path: str = 'bug-predictor/data/',
                         use_smote: bool = True,
                         use_pca: bool = True,
                         test_size: float = 0.2,
                         random_state: int = 42) -> Tuple[np.ndarray, np.ndarray]:
        """
        Pr√©pare les donn√©es de test
        
        Args:
            data_path: Chemin vers les donn√©es
            use_smote: Appliquer SMOTE
            use_pca: Appliquer PCA
            test_size: Proportion des donn√©es de test
            random_state: Seed pour la reproductibilit√©
            
        Returns:
            Tuple (X_test, y_test)
        """
        print("üìä Pr√©paration des donn√©es de test...")
        
        # 1. Chargement des donn√©es
        X_df, Y_df, _ = load_and_preprocess_data(data_path=data_path)
        
        # 2. Application de SMOTE (optionnel)
        if use_smote:
            X_res, Y_res = apply_smote(X_df, Y_df)
        else:
            X_res, Y_res = X_df, Y_df
        
        # 3. Encodage des labels
        y_encoded, label_encoder = encode_labels(Y_res)
        
        # 4. Split des donn√©es
        X_train, X_test, y_train, y_test = split_data(
            X_res.values, y_encoded, 
            test_size=test_size, 
            random_state=random_state
        )
        
        # 5. Chargement des pr√©processeurs sauvegard√©s
        scaler, pca = load_preprocessors(path=self.models_dir)
        
        # 6. Transformation des donn√©es de test
        X_test_scaled = scaler.transform(X_test)
        
        if use_pca and pca is not None:
            X_test_transformed = pca.transform(X_test_scaled)
            print(f"   ‚úì PCA appliqu√©: {X_test.shape[1]} ‚Üí {X_test_transformed.shape[1]} features")
        else:
            X_test_transformed = X_test_scaled
        
        self.test_data = {
            'X_test': X_test_transformed,
            'y_test': y_test,
            'label_encoder': label_encoder,
            'scaler': scaler,
            'pca': pca
        }
        
        print(f"   ‚úì Donn√©es de test pr√™tes: {X_test_transformed.shape[0]} √©chantillons")
        return X_test_transformed, y_test
    
    def load_saved_models(self, model_types: List[str] = None) -> Dict[str, BaseModel]:
        """
        Charge les mod√®les sauvegard√©s
        
        Args:
            model_types: Liste des types de mod√®les √† charger
            
        Returns:
            Dictionnaire des mod√®les charg√©s
        """
        print("\nüìÇ Chargement des mod√®les sauvegard√©s...")
        
        if model_types is None:
            model_types = [
                'LogisticRegression',
                'LSTM',
                'RandomForest',
                'SVM',
                'KNN',
                'NaiveBayes',
                'DecisionTree'
            ]
        
        self.loaded_models = {}
        
        # Chercher les fichiers de mod√®les
        model_files = {}
        for filename in os.listdir(self.models_dir):
            if filename.endswith('.joblib'):
                # Extraire le nom du mod√®le du filename
                for model_type in model_types:
                    if model_type.lower() in filename.lower():
                        model_files[model_type] = os.path.join(self.models_dir, filename)
                        break
        
        # Charger les mod√®les
        for model_type, model_path in model_files.items():
            try:
                print(f"   üîç Chargement de {model_type}...")
                
                # Charger avec joblib
                model_data = joblib.load(model_path)
                
                # V√©rifier si c'est un objet BaseModel
                if isinstance(model_data, dict) and 'model' in model_data:
                    # C'est un mod√®le sauvegard√© avec notre architecture
                    model = BaseModel("", {})
                    model.load(model_path)
                    self.loaded_models[model_type] = model
                    print(f"     ‚úÖ {model_type} charg√© ({model.model_name})")
                else:
                    # C'est un mod√®le scikit-learn direct
                    print(f"     ‚ö†Ô∏è {model_type} format ancien, cr√©ation wrapper...")
                    # Cr√©er un wrapper
                    from src.core.model_factory import ModelFactory
                    wrapper = ModelFactory.create_model(
                        model_type.lower().replace('model', ''),
                        **{}
                    )
                    wrapper.model = model_data
                    wrapper.trained = True
                    wrapper.model_name = model_type
                    self.loaded_models[model_type] = wrapper
                    
            except Exception as e:
                print(f"     ‚ùå Erreur chargement {model_type}: {e}")
                continue
        
        print(f"   ‚úì {len(self.loaded_models)} mod√®les charg√©s")
        return self.loaded_models
    
    def evaluate_single_model(self, model: BaseModel, model_name: str) -> Dict[str, Any]:
        """
        √âvalue un seul mod√®le
        
        Args:
            model: Mod√®le √† √©valuer
            model_name: Nom du mod√®le
            
        Returns:
            Dictionnaire des m√©triques
        """
        if self.test_data is None:
            raise ValueError("Donn√©es de test non pr√©par√©es. Appelez prepare_test_data() d'abord.")
        
        X_test = self.test_data['X_test']
        y_test = self.test_data['y_test']
        
        print(f"\n   üìä √âvaluation de {model_name}...")
        
        try:
            # √âvaluation standard
            metrics = evaluate_model_performance(
                model.model if hasattr(model, 'model') else model,
                X_test, y_test,
                model_name=model_name
            )
            
            # Si c'est un BaseModel, utiliser sa m√©thode evaluate
            if isinstance(model, BaseModel) and model.trained:
                model_metrics = model.evaluate(X_test, y_test)
                metrics.update(model_metrics)
            
            self.evaluation_results[model_name] = metrics
            print(f"     ‚úÖ {model_name}: Accuracy={metrics.get('accuracy', 0):.3f}, F1={metrics.get('f1_score', 0):.3f}")
            
            return metrics
            
        except Exception as e:
            print(f"     ‚ùå Erreur √©valuation {model_name}: {e}")
            return {}
    
    def evaluate_all_models(self) -> Dict[str, Dict[str, Any]]:
        """
        √âvalue tous les mod√®les charg√©s
        
        Returns:
            Dictionnaire des r√©sultats d'√©valuation
        """
        print("\nüî¨ √âvaluation de tous les mod√®les...")
        
        if not self.loaded_models:
            print("‚ö†Ô∏è Aucun mod√®le charg√©. Appelez load_saved_models() d'abord.")
            return {}
        
        self.evaluation_results = {}
        
        for model_name, model in self.loaded_models.items():
            self.evaluate_single_model(model, model_name)
        
        return self.evaluation_results
    
    def generate_comparison_report(self) -> pd.DataFrame:
        """
        G√©n√®re un rapport comparatif des mod√®les
        
        Returns:
            DataFrame avec les r√©sultats compar√©s
        """
        if not self.evaluation_results:
            print("‚ö†Ô∏è Aucun r√©sultat d'√©valuation. Appelez evaluate_all_models() d'abord.")
            return pd.DataFrame()
        
        # Cr√©er un DataFrame pour la comparaison
        comparison_data = []
        
        for model_name, metrics in self.evaluation_results.items():
            comparison_data.append({
                'Model': model_name,
                'Accuracy': metrics.get('accuracy', 0),
                'Precision': metrics.get('precision', 0),
                'Recall': metrics.get('recall', 0),
                'F1-Score': metrics.get('f1_score', 0),
                'ROC-AUC': metrics.get('roc_auc', 0),
                'Training_Time': self.loaded_models[model_name].training_time 
                    if hasattr(self.loaded_models[model_name], 'training_time') else None
            })
        
        df = pd.DataFrame(comparison_data)
        
        # Trier par F1-Score
        df = df.sort_values('F1-Score', ascending=False).reset_index(drop=True)
        
        return df
    
    def save_evaluation_results(self):
        """
        Sauvegarde les r√©sultats d'√©valuation
        """
        if not self.evaluation_results:
            print("‚ö†Ô∏è Aucun r√©sultat √† sauvegarder")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Sauvegarde des r√©sultats bruts
        raw_results_path = f"{EVALUATION_DIR}evaluation_raw_{timestamp}.json"
        with open(raw_results_path, 'w') as f:
            json.dump(self.evaluation_results, f, indent=2)
        
        # Sauvegarde du rapport comparatif
        comparison_df = self.generate_comparison_report()
        if not comparison_df.empty:
            csv_path = f"{EVALUATION_DIR}model_comparison_{timestamp}.csv"
            comparison_df.to_csv(csv_path, index=False)
            
            # Sauvegarde en format lisible
            report_path = f"{EVALUATION_DIR}evaluation_report_{timestamp}.txt"
            with open(report_path, 'w') as f:
                f.write("=" * 80 + "\n")
                f.write("üìä RAPPORT D'√âVALUATION DES MOD√àLES\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"Date d'√©valuation: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Nombre de mod√®les: {len(self.evaluation_results)}\n")
                f.write(f"√âchantillons de test: {self.test_data['X_test'].shape[0]}\n\n")
                
                f.write("üèÜ CLASSEMENT DES MOD√àLES (par F1-Score):\n")
                f.write("-" * 80 + "\n")
                
                for i, row in comparison_df.iterrows():
                    rank = "ü•á" if i == 0 else "ü•à" if i == 1 else "ü•â" if i == 2 else f"{i+1:2d}."
                    f.write(f"{rank} {row['Model']:25s}\n")
                    f.write(f"   F1-Score:    {row['F1-Score']:.4f}\n")
                    f.write(f"   Accuracy:    {row['Accuracy']:.4f}\n")
                    f.write(f"   Precision:   {row['Precision']:.4f}\n")
                    f.write(f"   Recall:      {row['Recall']:.4f}\n")
                    f.write(f"   ROC-AUC:     {row['ROC-AUC']:.4f}\n")
                    if row['Training_Time']:
                        f.write(f"   Training:    {row['Training_Time']:.2f}s\n")
                    f.write("\n")
        
        print(f"üíæ R√©sultats sauvegard√©s dans {EVALUATION_DIR}")
        return raw_results_path, csv_path
    
    def plot_model_comparison(self, save_plot: bool = True):
        """
        G√©n√®re des visualisations de comparaison des mod√®les
        
        Args:
            save_plot: Si True, sauvegarde les plots
        """
        if not self.evaluation_results:
            print("‚ö†Ô∏è Aucun r√©sultat √† visualiser")
            return
        
        comparison_df = self.generate_comparison_report()
        if comparison_df.empty:
            return
        
        # Configuration du style
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")
        
        # 1. Comparaison des m√©triques principales
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Comparaison des Performances des Mod√®les', fontsize=16, fontweight='bold')
        
        # F1-Score
        ax1 = axes[0, 0]
        bars1 = ax1.barh(comparison_df['Model'], comparison_df['F1-Score'])
        ax1.set_xlabel('F1-Score')
        ax1.set_title('F1-Score par Mod√®le')
        ax1.invert_yaxis()
        for bar in bars1:
            width = bar.get_width()
            ax1.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                    f'{width:.3f}', ha='left', va='center')
        
        # Accuracy
        ax2 = axes[0, 1]
        bars2 = ax2.barh(comparison_df['Model'], comparison_df['Accuracy'])
        ax2.set_xlabel('Accuracy')
        ax2.set_title('Accuracy par Mod√®le')
        ax2.invert_yaxis()
        for bar in bars2:
            width = bar.get_width()
            ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                    f'{width:.3f}', ha='left', va='center')
        
        # ROC-AUC
        ax3 = axes[1, 0]
        bars3 = ax3.barh(comparison_df['Model'], comparison_df['ROC-AUC'])
        ax3.set_xlabel('ROC-AUC')
        ax3.set_title('ROC-AUC par Mod√®le')
        ax3.invert_yaxis()
        for bar in bars3:
            width = bar.get_width()
            ax3.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                    f'{width:.3f}', ha='left', va='center')
        
        # Matrice des m√©triques
        ax4 = axes[1, 1]
        metrics_to_plot = comparison_df[['F1-Score', 'Accuracy', 'Precision', 'Recall']].T
        im = ax4.imshow(metrics_to_plot.values, aspect='auto', cmap='YlOrRd')
        ax4.set_xticks(range(len(comparison_df['Model'])))
        ax4.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')
        ax4.set_yticks(range(len(metrics_to_plot.index)))
        ax4.set_yticklabels(metrics_to_plot.index)
        ax4.set_title('Matrice des M√©triques')
        plt.colorbar(im, ax=ax4)
        
        plt.tight_layout()
        
        if save_plot:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_path = f"{EVALUATION_DIR}model_comparison_{timestamp}.png"
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            print(f"üìà Plot sauvegard√©: {plot_path}")
        
        plt.show()
        
        # 2. Radar chart pour les meilleurs mod√®les
        self._plot_radar_chart(comparison_df.head(5), save_plot)
    
    def _plot_radar_chart(self, top_models_df: pd.DataFrame, save_plot: bool = True):
        """
        G√©n√®re un radar chart pour les meilleurs mod√®les
        
        Args:
            top_models_df: DataFrame des meilleurs mod√®les
            save_plot: Si True, sauvegarde le plot
        """
        if len(top_models_df) < 2:
            return
        
        # Normaliser les m√©triques pour le radar chart
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
        normalized_data = []
        
        for metric in metrics:
            max_val = top_models_df[metric].max()
            min_val = top_models_df[metric].min()
            if max_val > min_val:
                normalized = (top_models_df[metric] - min_val) / (max_val - min_val)
            else:
                normalized = top_models_df[metric] * 0 + 0.5  # Valeur moyenne
            normalized_data.append(normalized.values)
        
        normalized_data = np.array(normalized_data)
        
        # Cr√©er le radar chart
        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
        
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # Fermer le cercle
        
        colors = plt.cm.Set1(np.linspace(0, 1, len(top_models_df)))
        
        for idx, (_, row) in enumerate(top_models_df.iterrows()):
            values = normalized_data[:, idx].tolist()
            values += values[:1]
            
            ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[idx])
            ax.fill(angles, values, alpha=0.1, color=colors[idx])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics)
        ax.set_ylim(0, 1)
        ax.set_title('Radar Chart - Comparaison des Meilleurs Mod√®les', fontsize=14, fontweight='bold')
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        if save_plot:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_path = f"{EVALUATION_DIR}radar_chart_{timestamp}.png"
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            print(f"üìà Radar chart sauvegard√©: {plot_path}")
        
        plt.show()
    
    def generate_detailed_report(self) -> str:
        """
        G√©n√®re un rapport d√©taill√© au format texte
        
        Returns:
            Rapport d√©taill√©
        """
        if not self.evaluation_results:
            return "‚ö†Ô∏è Aucun r√©sultat d'√©valuation disponible."
        
        report = []
        report.append("=" * 80)
        report.append("üìã RAPPORT D√âTAILL√â D'√âVALUATION")
        report.append("=" * 80)
        report.append(f"\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Mod√®les √©valu√©s: {len(self.evaluation_results)}")
        
        if self.test_data:
            report.append(f"Taille du jeu de test: {self.test_data['X_test'].shape[0]} √©chantillons")
            report.append(f"Nombre de features: {self.test_data['X_test'].shape[1]}")
        
        report.append("\n" + "=" * 80)
        report.append("üìä R√âSULTATS PAR MOD√àLE")
        report.append("=" * 80)
        
        for model_name, metrics in self.evaluation_results.items():
            report.append(f"\nüîπ {model_name}")
            report.append(f"   Accuracy:    {metrics.get('accuracy', 0):.4f}")
            report.append(f"   Precision:   {metrics.get('precision', 0):.4f}")
            report.append(f"   Recall:      {metrics.get('recall', 0):.4f}")
            report.append(f"   F1-Score:    {metrics.get('f1_score', 0):.4f}")
            report.append(f"   ROC-AUC:     {metrics.get('roc_auc', 0):.4f}")
            
            # Matrice de confusion
            if 'confusion_matrix' in metrics:
                cm = metrics['confusion_matrix']
                if isinstance(cm, list) and len(cm) == 2:
                    report.append(f"   Matrice de confusion:")
                    report.append(f"      [[{cm[0][0]:4d}  {cm[0][1]:4d}]")
                    report.append(f"       [{cm[1][0]:4d}  {cm[1][1]:4d}]]")
        
        # Recommandations
        report.append("\n" + "=" * 80)
        report.append("üí° RECOMMANDATIONS")
        report.append("=" * 80)
        
        # Trouver le meilleur mod√®le par m√©trique
        best_f1 = max(self.evaluation_results.items(), 
                     key=lambda x: x[1].get('f1_score', 0))
        best_accuracy = max(self.evaluation_results.items(), 
                           key=lambda x: x[1].get('accuracy', 0))
        best_roc = max(self.evaluation_results.items(), 
                      key=lambda x: x[1].get('roc_auc', 0))
        
        report.append(f"\nüéØ Meilleur mod√®le F1-Score: {best_f1[0]} ({best_f1[1].get('f1_score', 0):.4f})")
        report.append(f"üéØ Meilleur mod√®le Accuracy: {best_accuracy[0]} ({best_accuracy[1].get('accuracy', 0):.4f})")
        report.append(f"üéØ Meilleur mod√®le ROC-AUC: {best_roc[0]} ({best_roc[1].get('roc_auc', 0):.4f})")
        
        # Suggestions bas√©es sur les performances
        report.append("\nüìã Suggestions:")
        
        if best_f1[1].get('f1_score', 0) > 0.8:
            report.append("   ‚úÖ Excellentes performances! Le mod√®le est pr√™t pour la production.")
        elif best_f1[1].get('f1_score', 0) > 0.6:
            report.append("   ‚ö†Ô∏è Bonnes performances. Peut √™tre am√©lior√© avec plus de donn√©es.")
        else:
            report.append("   ‚ùå Performances √† am√©liorer. Consid√©rez:")
            report.append("       - Collecter plus de donn√©es")
            report.append("       - R√©√©quilibrer les classes")
            report.append("       - Essayer d'autres features")
        
        return "\n".join(report)

def evaluate_all_saved_models():
    """
    Fonction principale pour √©valuer tous les mod√®les sauvegard√©s
    """
    print("üöÄ D√©marrage du pipeline d'√©valuation...")
    
    # Cr√©er l'√©valuateur
    evaluator = ModelEvaluator()
    
    # 1. Pr√©parer les donn√©es de test
    X_test, y_test = evaluator.prepare_test_data()
    
    # 2. Charger les mod√®les
    models = evaluator.load_saved_models()
    
    if not models:
        print("‚ùå Aucun mod√®le charg√©. V√©rifiez le r√©pertoire des mod√®les.")
        return
    
    # 3. √âvaluer tous les mod√®les
    results = evaluator.evaluate_all_models()
    
    # 4. G√©n√©rer et sauvegarder les rapports
    print("\nüìã G√©n√©ration des rapports...")
    
    # Rapport d√©taill√©
    detailed_report = evaluator.generate_detailed_report()
    print(detailed_report)
    
    # Sauvegarde des r√©sultats
    evaluator.save_evaluation_results()
    
    # Visualisations
    print("\nüé® G√©n√©ration des visualisations...")
    evaluator.plot_model_comparison(save_plot=True)
    
    # Afficher le classement
    comparison_df = evaluator.generate_comparison_report()
    if not comparison_df.empty:
        print("\n" + "=" * 80)
        print("üèÜ CLASSEMENT FINAL")
        print("=" * 80)
        print(comparison_df.to_string(index=False))
    
    print("\n" + "=" * 80)
    print("‚úÖ √âVALUATION TERMIN√âE AVEC SUCC√àS!")
    print("=" * 80)
    
    return evaluator

def evaluate_specific_models(model_names: List[str]):
    """
    √âvalue des mod√®les sp√©cifiques
    
    Args:
        model_names: Liste des noms des mod√®les √† √©valuer
    """
    evaluator = ModelEvaluator()
    
    # Pr√©parer les donn√©es
    evaluator.prepare_test_data()
    
    # Charger les mod√®les sp√©cifiques
    evaluator.load_saved_models(model_names)
    
    # √âvaluer
    results = evaluator.evaluate_all_models()
    
    # G√©n√©rer rapport
    report = evaluator.generate_detailed_report()
    print(report)
    
    return evaluator

def compare_two_models(model1_path: str, model2_path: str, 
                      data_path: str = 'bug-predictor/data/'):
    """
    Compare deux mod√®les sp√©cifiques
    
    Args:
        model1_path: Chemin vers le premier mod√®le
        model2_path: Chemin vers le second mod√®le
        data_path: Chemin vers les donn√©es
    """
    print("‚öñÔ∏è Comparaison de deux mod√®les...")
    
    # Charger les mod√®les
    model1 = joblib.load(model1_path)
    model2 = joblib.load(model2_path)
    
    # Pr√©parer les donn√©es
    evaluator = ModelEvaluator()
    X_test, y_test = evaluator.prepare_test_data(data_path=data_path)
    
    # √âvaluer chaque mod√®le
    results = {}
    
    for name, model in [('Model 1', model1), ('Model 2', model2)]:
        try:
            metrics = evaluate_model_performance(model, X_test, y_test, model_name=name)
            results[name] = metrics
            print(f"‚úÖ {name}: F1={metrics.get('f1_score', 0):.3f}")
        except Exception as e:
            print(f"‚ùå Erreur avec {name}: {e}")
    
    # Comparaison
    if len(results) == 2:
        print("\n" + "=" * 60)
        print("üìä COMPARAISON")
        print("=" * 60)
        
        for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']:
            val1 = results.get('Model 1', {}).get(metric, 0)
            val2 = results.get('Model 2', {}).get(metric, 0)
            
            if val1 > val2:
                winner = "Model 1"
                diff = val1 - val2
            else:
                winner = "Model 2"
                diff = val2 - val1
            
            print(f"{metric:12s}: Model 1={val1:.3f} | Model 2={val2:.3f} | "
                  f"Gagnant: {winner} (+{diff:.3f})")

if __name__ == '__main__':
    """
    Exemple d'utilisation
    """
    
    # Option 1: √âvaluer tous les mod√®les
    evaluator = evaluate_all_saved_models()
    
    # Option 2: √âvaluer des mod√®les sp√©cifiques
    # evaluator = evaluate_specific_models(['RandomForest', 'LogisticRegression'])
    
    # Option 3: Comparer deux mod√®les sp√©cifiques
    # compare_two_models(
    #     'bug-predictor/models/RandomForestModel_20250101_120000.joblib',
    #     'bug-predictor/models/LogisticRegressionModel_20250101_120000.joblib'
    # )